The Arxiv dataset was downloaded from Kaggle (https://www.kaggle.com/datasets/Cornell-University/arxiv), the Wiki dump from https://dumps.wikimedia.org/, and the LCC from https://wortschatz.uni-leipzig.de/en/download/ . Arxiv was preprocessed with arxiv_get_abstracts.py, the Wikipediadump with wiki_remove_xml.py and wiki_clean_wiki.py , the GPT4 abstracts were made with ai_write_abstract.py. Then count_keywords.py was applied to the different datasets, to get the results presented in Section 4 and Appendix A. 
